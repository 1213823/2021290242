{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9adfe10c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.parallel\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.distributed as dist\n",
    "import torch.nn.functional as F\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')  # \"error\", \"ignore\", \"always\", \"default\", \"module\" or \"once\"\n",
    "import torch.optim\n",
    "import torch.utils.data\n",
    "import models\n",
    "import data_got\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score, accuracy_score\n",
    "\n",
    "parser = argparse.ArgumentParser(description='PyTorch Training')\n",
    "\n",
    "parser.add_argument('-j', '--workers', default=4, type=int, metavar='N',\n",
    "                    help='number of data loading workers (default: 4)')\n",
    "parser.add_argument('-b', '--batch-size', default=45, type=int,\n",
    "                    metavar='N', help='mini-batch size (default: 64)')\n",
    "parser.add_argument('-c', '--checkpoint', default='imprint_checkpoint', type=str, metavar='PATH',\n",
    "                    help='path to save checkpoint (default: imprint_checkpoint)')\n",
    "parser.add_argument('--model', default='samp45f_pretrain_checkpoint/model_best.pth.tar', type=str, metavar='PATH',\n",
    "                    help='path to model (default: none)')\n",
    "parser.add_argument('--random', action='store_true', help='whether use random novel weights')\n",
    "parser.add_argument('--num_sample', default=5, type=int,\n",
    "                    metavar='N', help='number of novel sample (default: 1)')\n",
    "parser.add_argument('--test-novel-only', action='store_true', help='whether only test on novel classes')\n",
    "parser.add_argument('--aug', action='store_true', help='whether use data augmentation during training')\n",
    "parser.add_argument('--lstm_hid_dim', default=150, type=int, metavar='N',\n",
    "                    help='lstm_hid_dim')\n",
    "parser.add_argument('--num_class', default=45, type=int, metavar='N',\n",
    "                    help='the number of class')\n",
    "parser.add_argument('--epochs', default=7 ,type=int, metavar='N',\n",
    "                    help='number of total epochs to run')\n",
    "parser.add_argument('--Bsamp_freq', default=60, type=int, metavar='N',\n",
    "                    help='number of total epochs to run')\n",
    "parser.add_argument('--Nsamp_freq', default=30, type=int, metavar='N',\n",
    "                    help='number of total epochs to run')\n",
    "parser.add_argument('--samp_freq', default=30, type=int, metavar='N',\n",
    "                    help='number of total epochs to run')\n",
    "parser.add_argument('--lr', '--learning-rate', default=0.00001, type=float,\n",
    "                    metavar='LR', help='initial learning rate')\n",
    "parser.add_argument('--ensemble-G', default=1, type=int, metavar='N',\n",
    "                    help='ensemble G for EUR-Lex dataset')\n",
    "\n",
    "\n",
    "def main():\n",
    "    global args, best_micro\n",
    "    args = parser.parse_args(args=[])\n",
    "    \n",
    "    base_transf, embed = data_got.one_sample_base2avg(batch_size=args.batch_size, sample_num=args.num_sample, samp_freq=args.Bsamp_freq, num_class=args.num_class)\n",
    "    Ftest_loader, novel_loader, novelall_loader = data_got.Nload_data(batch_size=args.batch_size, sample_num=args.num_sample, samp_freq=args.Nsamp_freq, num_class=args.num_class)\n",
    "    embed = torch.from_numpy(embed).float()\n",
    "    model = models.Net(embed, args.lstm_hid_dim, num_classes=args.num_class).cuda()\n",
    "\n",
    "    print('==> Reading from model checkpoint..')\n",
    "    assert os.path.isfile(args.model), 'Error: no model checkpoint directory found!'\n",
    "    checkpoint = torch.load(args.model)\n",
    "    model.load_state_dict(checkpoint['state_dict'])\n",
    "    print(\"=> loaded model checkpoint '{}' (epoch {})\"\n",
    "          .format(args.model, checkpoint['epoch']))\n",
    "    cudnn.benchmark = True\n",
    "    real_weight = model.classifier.fc.weight.data\n",
    "\n",
    "    criterion = nn.MSELoss()\n",
    "    trans_model = models.Transfer().cuda()\n",
    "    optimizer = torch.optim.Adam(trans_model.parameters(), lr=0.013, betas=(0.9, 0.99))\n",
    "\n",
    "    for epoch in range(args.epochs):\n",
    "        train_loss = train(base_transf, trans_model, model, criterion, optimizer, real_weight)\n",
    "        print(\"loss\", train_loss)\n",
    "    tail_weight = imprint(novel_loader, model, trans_model)\n",
    "\n",
    "    output_all = []\n",
    "    F1 = np.zeros(54)\n",
    "    for i in range(args.ensemble_G):  # 使用args.ensemble_G\n",
    "        print(\"ensemble start!!!!!!!! this is classifier\", i)\n",
    "        model.classifier.fc.weight.data = tail_weight[i]\n",
    "        output = validate(Ftest_loader, model)\n",
    "        output_all.append(output)\n",
    "    output_all = (torch.sum(torch.tensor(output_all), 0)) / args.ensemble_G\n",
    "    output_all[output_all > 0.5] = 1\n",
    "    output_all[output_all <= 0.5] = 0\n",
    "    for l in range(54):\n",
    "        F1[l] = f1_score(test_y[:, l], output_all[:, l], average='binary')\n",
    "    print(\"each class result f1!!!!!!!!!!!!!!\")\n",
    "    print(F1)\n",
    "\n",
    "    \n",
    "\n",
    "def train(train_loader, trans_model,model, criterion,optimizer,real_weight):\n",
    "    trans_model.train()\n",
    "    base_rep=[]\n",
    "    losses=[]\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (input, target) in enumerate(train_loader):\n",
    "            input = input.cuda()\n",
    "            output = model.extract(input)\n",
    "            base_rep.extend(output.cpu().numpy())\n",
    "    base_rep =np.array(base_rep)\n",
    "    base_rep= torch.from_numpy(base_rep).cuda()\n",
    "    new_weight = torch.zeros(2700, 128).cuda()\n",
    "    j = 0\n",
    "    for i in range(2700):\n",
    "        tmp =base_rep[j:j + args.num_sample]\n",
    "        tmp = torch.sum(tmp, 0) / args.num_sample\n",
    "        new_weight[i] = tmp / tmp.norm(p=2)\n",
    "        j = j + args.num_sample\n",
    "    e=0\n",
    "    for h in range(args.Bsamp_freq):\n",
    "        doc_avg=new_weight[e:e+args.num_class,:]\n",
    "        e=e+args.num_class\n",
    "        output = trans_model(doc_avg)\n",
    "        loss = criterion(output, real_weight)\n",
    "        losses.append(float(loss))\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    avg_loss = np.mean(losses)\n",
    "    return avg_loss\n",
    "\n",
    "def imprint(novel_loader, model, trans_model):\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (input, target) in enumerate(novel_loader):\n",
    "            input = input.cuda()\n",
    "            output = model.extract(input)\n",
    "            if batch_idx == 0:\n",
    "                output_stack = output\n",
    "                target_stack = target\n",
    "            else:\n",
    "                output_stack = torch.cat((output_stack, output), 0)\n",
    "                target_stack = torch.cat((target_stack, target), 0)\n",
    "    new_weight = torch.zeros(270, 128).cuda()\n",
    "    j=0\n",
    "    for i in range(270):\n",
    "        tmp = output_stack[j:j + args.num_sample]\n",
    "        tmp = torch.sum(tmp, 0) / args.num_sample\n",
    "        new_weight[i] = tmp / tmp.norm(p=2)\n",
    "        j = j + args.num_sample\n",
    "    tail_real=trans_model.transfor(new_weight)\n",
    "    tail_sampfre=[]\n",
    "    j=0\n",
    "    for i in range(args.Nsamp_freq):\n",
    "        qq = tail_real[j:j + 9]\n",
    "        j=j+9\n",
    "        weight = torch.cat([model.classifier.fc.weight.data, qq])\n",
    "        tail_sampfre.append(weight)\n",
    "    print('imprint done')\n",
    "    return tail_sampfre\n",
    "\n",
    "\n",
    "def validate(val_loader, model):\n",
    "    one_iteration=[]\n",
    "    F1 = np.zeros(54)\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "\n",
    "        for batch_idx, (input, target) in enumerate(val_loader):\n",
    "            input = input.cuda()\n",
    "            target = target.cuda()\n",
    "            output = model(input)\n",
    "            target = target.data.cpu().float()\n",
    "            output = output.data.cpu()\n",
    "            one_iteration.extend(output.numpy())\n",
    "            for l in range(54):\n",
    "                F1[l] += f1_score(target[:, l], output[:, l], average='binary')\n",
    "        np.set_printoptions(formatter={'float': '{: 0.4}'.format})\n",
    "        print('the result of F1: \\n', F1/len(val_loader))\n",
    "        return one_iteration\n",
    "\n",
    "\n",
    "\n",
    "def fine_tuning(train_loader, model, criterion, optimizer):\n",
    "    batch_time = AverageMeter()\n",
    "    data_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "    microF1 = AverageMeter()\n",
    "    macroF1 = AverageMeter()\n",
    "    model.train()\n",
    "\n",
    "    end = time.time()\n",
    "    # bar = Bar('Training', max=len(train_loader))\n",
    "    for batch_idx, (input, target) in enumerate(train_loader):\n",
    "        # measure data loading time\n",
    "        data_time.update(time.time() - end)\n",
    "        input = input.cuda()\n",
    "        target = target.cuda()\n",
    "        output = model(input)\n",
    "        loss = criterion(output, target.float())\n",
    "        target = target.data.cpu().float()\n",
    "        output=output.data.cpu()\n",
    "        #\n",
    "        # micro,macro = calc_f1( target,  output)\n",
    "\n",
    "\n",
    "        losses.update(loss.item(), input.size(0))\n",
    "        # microF1.update(micro.item(), input.size(0))\n",
    "        # macroF1.update(macro.item(), input.size(0))\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # measure elapsed time\n",
    "        batch_time.update(time.time() - end)\n",
    "        end = time.time()\n",
    "\n",
    "        # model.weight_norm()\n",
    "        # plot progress\n",
    "    #     bar.suffix = '({batch}/{size}) Data: {data:.3f}s | Batch: {bt:.3f}s | Total: {total:} | ETA: {eta:} | Loss: {loss:.4f} | Micro-f1: {microF1: .4f} |Macro-f1: {macroF1: .4f}'.format(\n",
    "    #         batch=batch_idx + 1,\n",
    "    #         size=len(train_loader),\n",
    "    #         data=data_time.val,\n",
    "    #         bt=batch_time.val,\n",
    "    #         total=bar.elapsed_td,\n",
    "    #         eta=bar.eta_td,\n",
    "    #         loss=losses.avg,\n",
    "    #         microF1=microF1.avg,\n",
    "    #         macroF1=macroF1.avg,\n",
    "    #     )\n",
    "    #     bar.next()\n",
    "    # bar.finish()\n",
    "    return (losses.avg, microF1.avg, macroF1.avg)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d5cf6b4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "F1 = np.zeros(54)\n",
    "print(F1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45bc7f90",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
